{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Augmented Tensor Factorization\n",
    "\n",
    "**Published**: November 12, 2020\n",
    "\n",
    "**Author**: Yixian Chen , Xinyu Chen \n",
    "\n",
    "To overcome the problem of missing values within multivariate time series data, this model takes into account low-rank tensor structure by folding data along day dimension. For an in-depth discussion of BATF, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Zhaocheng He, Yixian Chen, Yuhuan Lu, Jiawei Wang (2019). <b>Missing traffic data imputation and pattern discovery with a Bayesian augmented tensor factorization model</b>. Transportation Research Part C: Emerging Technologies, 104: 66-77. <a href=\"https://doi.org/10.1016/j.trc.2019.03.003\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset ,ConcatDataset\n",
    "sys.path.append(r'D:\\WorkPath\\Models\\SAGAN')\n",
    "from MyDataSet import MultiMaskTimeSeriesDataset\n",
    "\n",
    "from numpy.random import multivariate_normal as mvnrnd\n",
    "from scipy.stats import wishart\n",
    "from numpy.random import normal as normrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.linalg import solve as solve\n",
    "from numpy.linalg import cholesky as cholesky_lower\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu\n",
    "\n",
    "def cp_combine(var):\n",
    "    return np.einsum('is, js, ts -> ijt', var[0], var[1], var[2])\n",
    "\n",
    "## 1st solution\n",
    "def vec_combine(vector):\n",
    "    tensor = 0\n",
    "    d = len(vector)\n",
    "    for i in range(d):\n",
    "        ax = [len(vector[i]) if j == i else 1 for j in range(d)]\n",
    "        tensor = tensor + vector[i].reshape(ax, order = 'F')\n",
    "    return tensor\n",
    "\n",
    "## 2nd solution\n",
    "def vec_combine(vector):\n",
    "    return (vector[0][:, np.newaxis, np.newaxis] + vector[1][np.newaxis, :, np.newaxis]\n",
    "            + vector[2][np.newaxis, np.newaxis, :])\n",
    "    \n",
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')\n",
    "\n",
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat\n",
    "\n",
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])\n",
    "\n",
    "def compute_mse(var, var_hat):\n",
    "    return np.sum((var - var_hat) ** 2) / var.shape[0]\n",
    "\n",
    "def sample_global_mu(mu_sparse, pos_obs, tau_eps, tau0 = 1):\n",
    "    tau_tilde = 1 / (tau_eps * len(pos_obs[0]) + tau0)\n",
    "    mu_tilde = tau_eps * np.sum(mu_sparse) * tau_tilde\n",
    "    return np.random.normal(mu_tilde, np.sqrt(tau_tilde))\n",
    "\n",
    "def sample_bias_vector(bias_sparse,vector, factor, bias, ind, dim, k, tau_eps, tau0 = 1):\n",
    "    for k in range(len(dim)):\n",
    "        idx = tuple(filter(lambda x: x != k, range(len(dim))))\n",
    "        temp = vector.copy()\n",
    "        temp[k] = np.zeros((dim[k]))\n",
    "        tau_tilde = 1 / (tau_eps * bias[k] + tau0)\n",
    "        mu_tilde = tau_eps * np.sum(ind * (bias_sparse - vec_combine(temp)), axis = idx) * tau_tilde\n",
    "        vector[k] = np.random.normal(mu_tilde, np.sqrt(tau_tilde))\n",
    "    return vector\n",
    "\n",
    "def sample_factor(tau_sparse, factor, ind, dim, k, tau_eps, beta0 = 1):\n",
    "    dim, rank = factor[k].shape\n",
    "    dim = factor[k].shape[0]\n",
    "    factor_bar = np.mean(factor[k], axis = 0)\n",
    "    temp = dim / (dim + beta0)\n",
    "    var_mu_hyper = temp * factor_bar\n",
    "    var_W_hyper = inv(np.eye(rank) + cov_mat(factor[k], factor_bar) + temp * beta0 * np.outer(factor_bar, factor_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim + rank, scale = var_W_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(var_mu_hyper, (dim + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    idx = list(filter(lambda x: x != k, range(len(factor))))\n",
    "    var1 = kr_prod(factor[idx[1]], factor[idx[0]]).T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ ten2mat(tau_eps * ind, k).T).reshape([rank, rank, dim]) + var_Lambda_hyper[:, :, np.newaxis]\n",
    "    var4 = var1 @ ten2mat(tau_sparse, k).T + (var_Lambda_hyper @ var_mu_hyper)[:, np.newaxis]\n",
    "    for i in range(dim):\n",
    "        factor[k][i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "    return factor[k]\n",
    "\n",
    "def sample_precision_tau(error_tensor, pos_obs):\n",
    "    var_alpha = 1e-6 + 0.5 * len(pos_obs[0])\n",
    "    var_beta = 1e-6 + 0.5 * np.linalg.norm(error_tensor, 2) ** 2\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)\n",
    "\n",
    "\n",
    "def BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter):\n",
    "    \"\"\"Bayesian Augmented Tensor Factorization (BATF) with Gibbs sampling.\"\"\"\n",
    "\n",
    "    dim = np.array(sparse_tensor.shape)\n",
    "    rank = factor[0].shape[1]\n",
    "    if np.isnan(sparse_tensor).any() == False:\n",
    "        ind = sparse_tensor != 0\n",
    "        pos_obs = np.where(ind)\n",
    "        pos_test = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "    elif np.isnan(sparse_tensor).any() == True:\n",
    "        pos_test = np.where((dense_tensor != 0) & (np.isnan(sparse_tensor)))\n",
    "        ind = ~np.isnan(sparse_tensor)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_tensor[np.isnan(sparse_tensor)] = 0\n",
    "    num_obs = len(pos_obs[0])\n",
    "    dense_test = dense_tensor[pos_test]\n",
    "    del dense_tensor\n",
    "\n",
    "    show_iter = 50\n",
    "    tau_eps = 1\n",
    "    bias = []\n",
    "    for k in range(len(dim)):\n",
    "        idx = tuple(filter(lambda x: x != k, range(len(dim))))\n",
    "        bias.append(np.sum(ind, axis = idx))\n",
    "    temp = cp_combine(factor)\n",
    "    temp_hat = np.zeros(len(pos_test[0]))\n",
    "    tensor_hat_plus = np.zeros(dim)\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        temp = sparse_tensor - temp\n",
    "        mu_glb = sample_global_mu(temp[pos_obs] - vec_combine(vector)[pos_obs], pos_obs, tau_eps)\n",
    "        vector = sample_bias_vector(temp - mu_glb,vector, factor, bias, ind, dim, k, tau_eps )\n",
    "        del temp\n",
    "        tau_sparse = tau_eps * ind * (sparse_tensor - mu_glb - vec_combine(vector))\n",
    "        for k in range(len(dim)):\n",
    "            factor[k] = sample_factor(tau_sparse, factor, ind, dim, k, tau_eps)\n",
    "        temp = cp_combine(factor)\n",
    "        tensor_hat = mu_glb + vec_combine(vector) + temp\n",
    "        temp_hat += tensor_hat[pos_test]\n",
    "        tau_eps = sample_precision_tau(sparse_tensor[pos_obs] - tensor_hat[pos_obs], pos_obs)\n",
    "        if it + 1 > burn_iter:\n",
    "            tensor_hat_plus += tensor_hat\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat)))\n",
    "            print('MSE: {:.6}'.format(compute_mse(dense_test, temp_hat)))\n",
    "            \n",
    "            temp_hat = np.zeros(len(pos_test[0]))\n",
    "            print()\n",
    "    tensor_hat = tensor_hat_plus / gibbs_iter \n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, tensor_hat[pos_test])))\n",
    "    print('Imputation MSE: {:.6}'.format(compute_mse(dense_test, tensor_hat[pos_test])))\n",
    "    print()\n",
    "\n",
    "    return tensor_hat, mu_glb, vector, factor ,compute_mse(dense_test, tensor_hat[pos_test]) ,compute_mape(dense_test, tensor_hat[pos_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BATF(data : MultiMaskTimeSeriesDataset, iter ,history_len = 20):\n",
    "    test = np.random.randint(history_len*data.num_masks, len(data)-1 , iter)\n",
    "    total_MSE = []\n",
    "    total_MAE = []\n",
    "    for i in range(iter):\n",
    "        index = test[i]\n",
    "\n",
    "        dense_tensor = data.get_historical_data(index, history_len)[0]\n",
    "        masks = data.get_historical_data(index, history_len)[1][:,index % data.num_masks,:,:]\n",
    "        sparse_tensor = dense_tensor * masks\n",
    "        \n",
    "        dense_tensor = dense_tensor.transpose(2,0,1)\n",
    "        sparse_tensor = sparse_tensor.transpose(2,0,1)\n",
    "        dim = dense_tensor.shape\n",
    "\n",
    "        start = time.time()\n",
    "        dim = np.array(sparse_tensor.shape)\n",
    "        rank = 20\n",
    "        vector = []\n",
    "        factor = []\n",
    "        for k in range(len(dim)):\n",
    "            vector.append(0.1 * np.random.randn(dim[k],))\n",
    "            factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "        burn_iter = 50\n",
    "        gibbs_iter = 50\n",
    "        _,_,_,_, total_MSE, total_MAPE=BATF_Gibbs(dense_tensor, sparse_tensor, vector, factor, burn_iter, gibbs_iter)\n",
    "        end = time.time()\n",
    "        print('Running time: %d seconds'%(end - start))\n",
    "    return total_MSE,total_MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = r'D:\\WorkPath\\Models\\ImputeFormer'\n",
    "test_path = os.path.join(project_path , r'Data\\source_test_PEMS04') \n",
    "test_files = os.listdir(test_path)\n",
    "test_files = [os.path.join(test_path, file) for file in test_files]\n",
    "\n",
    "test_record = {'data_name':[],'MSE_test_loss':[] , 'MAPE_test_loss':[]}\n",
    "\n",
    "for file_path in test_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "\n",
    "    total_MSE,total_MAE=BATF(test_data,5 , 5)\n",
    "    test_record['data_name'].append(file_path)\n",
    "    test_record['MSE_test_loss'].append(np.mean(total_MSE))\n",
    "    test_record['MAPE_test_loss'].append(np.mean(total_MAE))\n",
    "test_record = pd.DataFrame(test_record)\n",
    "test_record['route']=test_record['data_name'].apply(lambda x :x.split('_')[5])\n",
    "test_record['start']=test_record['data_name'].apply(lambda x :x.split('_')[-3])\n",
    "test_record['miss_rate']=test_record['data_name'].apply(lambda x :x.split('_')[-2])\n",
    "test_record['type']=test_record['data_name'].apply(lambda x :x.split('_')[-1][:-4])\n",
    "test_record=test_record[['route','start','miss_rate','type','MSE_test_loss','MAPE_test_loss']]\n",
    "test_record=test_record.sort_values(['route','type','start'])\n",
    "test_record"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepGAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
