{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Gaussian CP decomposition\n",
    "\n",
    "**Published**: September 30, 2020\n",
    "\n",
    "**Author**: Xinyu Chen [[**GitHub homepage**](https://github.com/xinychen)]\n",
    "\n",
    "**Download**: This Jupyter notebook is at our GitHub repository. If you want to evaluate the code, please download the notebook from the [**transdim**](https://github.com/xinychen/transdim/blob/master/imputer/BGCP.ipynb) repository.\n",
    "\n",
    "This notebook shows how to implement the Bayesian Gaussian CP decomposition (BGCP) model on some real-world data sets. In the following, we will discuss:\n",
    "\n",
    "- What the Bayesian Gaussian CP decomposition is.\n",
    "\n",
    "- How to implement BGCP mainly using Python `numpy` with high efficiency.\n",
    "\n",
    "- How to make imputation on some real-world spatiotemporal datasets.\n",
    "\n",
    "To overcome the problem of missing values within multivariate time series data, this model takes into account low-rank tensor structure by folding data along day dimension. For an in-depth discussion of BGCP, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Zhaocheng He, Lijun Sun (2019). <b>A Bayesian tensor decomposition approach for spatiotemporal traffic data imputation</b>. Transportation Research Part C: Emerging Technologies, 98: 73-84. <a href=\"https://doi.org/10.1016/j.trc.2018.11.003\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal as mvnrnd\n",
    "from scipy.stats import wishart\n",
    "from numpy.random import normal as normrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.linalg import solve as solve\n",
    "from numpy.linalg import cholesky as cholesky_lower\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset ,ConcatDataset\n",
    "sys.path.append(r'D:\\WorkPath\\Models\\SAGAN')\n",
    "from MyDataSet import MultiMaskTimeSeriesDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu\n",
    "    \n",
    "def cp_combine(var):\n",
    "    return np.einsum('is, js, ts -> ijt', var[0], var[1], var[2])\n",
    "\n",
    "factor = [np.array([[1, 2], [3, 4]]), np.array([[1, 3], [2, 4], [5, 6]]), \n",
    "          np.array([[1, 5], [2, 6], [3, 7], [4, 8]])]\n",
    "print(cp_combine(factor))\n",
    "print()\n",
    "print('tensor size:')\n",
    "print(cp_combine(factor).shape)\n",
    "\n",
    "\n",
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')\n",
    "\n",
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat\n",
    "\n",
    "def sample_factor(tau_sparse_tensor, tau_ind, factor, k, beta0 = 1):\n",
    "    dim, rank = factor[k].shape\n",
    "    dim = factor[k].shape[0]\n",
    "    factor_bar = np.mean(factor[k], axis = 0)\n",
    "    temp = dim / (dim + beta0)\n",
    "    var_mu_hyper = temp * factor_bar\n",
    "    var_W_hyper = inv(np.eye(rank) + cov_mat(factor[k], factor_bar) + temp * beta0 * np.outer(factor_bar, factor_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim + rank, scale = var_W_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(var_mu_hyper, (dim + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    idx = list(filter(lambda x: x != k, range(len(factor))))\n",
    "    var1 = kr_prod(factor[idx[1]], factor[idx[0]]).T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ ten2mat(tau_ind, k).T).reshape([rank, rank, dim]) + var_Lambda_hyper[:, :, np.newaxis]\n",
    "    var4 = var1 @ ten2mat(tau_sparse_tensor, k).T + (var_Lambda_hyper @ var_mu_hyper)[:, np.newaxis]\n",
    "    for i in range(dim):\n",
    "        factor[k][i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "    return factor[k]\n",
    "\n",
    "def sample_precision_tau(sparse_tensor, tensor_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_tensor - tensor_hat) ** 2) * ind)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)\n",
    "\n",
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])\n",
    "\n",
    "def compute_mse(var, var_hat):\n",
    "    return  np.sum((var - var_hat) ** 2) / var.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGCP(dense_tensor, sparse_tensor, factor, burn_iter, gibbs_iter):\n",
    "    \"\"\"Bayesian Gaussian CP (BGCP) decomposition.\"\"\"\n",
    "    \n",
    "    dim = np.array(sparse_tensor.shape)\n",
    "    rank = factor[0].shape[1]\n",
    "    if np.isnan(sparse_tensor).any() == False:\n",
    "        ind = sparse_tensor != 0\n",
    "        pos_obs = np.where(ind)\n",
    "        pos_test = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "    elif np.isnan(sparse_tensor).any() == True:\n",
    "        pos_test = np.where((dense_tensor != 0) & (np.isnan(sparse_tensor)))\n",
    "        ind = ~np.isnan(sparse_tensor)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_tensor[np.isnan(sparse_tensor)] = 0\n",
    "    show_iter = 200\n",
    "    tau = 1\n",
    "    factor_plus = []\n",
    "    for k in range(len(dim)):\n",
    "        factor_plus.append(np.zeros((dim[k], rank)))\n",
    "    temp_hat = np.zeros(dim)\n",
    "    tensor_hat_plus = np.zeros(dim)\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        tau_ind = tau * ind\n",
    "        tau_sparse_tensor = tau * sparse_tensor\n",
    "        for k in range(len(dim)):\n",
    "            factor[k] = sample_factor(tau_sparse_tensor, tau_ind, factor, k)\n",
    "        tensor_hat = cp_combine(factor)\n",
    "        temp_hat += tensor_hat\n",
    "        tau = sample_precision_tau(sparse_tensor, tensor_hat, ind)\n",
    "        if it + 1 > burn_iter:\n",
    "            factor_plus = [factor_plus[k] + factor[k] for k in range(len(dim))]\n",
    "            tensor_hat_plus += tensor_hat\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], temp_hat[pos_test])))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], temp_hat[pos_test])))\n",
    "            temp_hat = np.zeros(sparse_tensor.shape)\n",
    "            print()\n",
    "    factor = [i / gibbs_iter for i in factor_plus]\n",
    "    tensor_hat = tensor_hat_plus / gibbs_iter\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_mse(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "    print()\n",
    "    \n",
    "    return tensor_hat, factor ,compute_mse(dense_tensor[pos_test], tensor_hat[pos_test]) ,compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGCP_T(data : MultiMaskTimeSeriesDataset, iter ,history_len = 20):\n",
    "\n",
    "    test = np.random.randint(history_len*data.num_masks, len(data)-1 , iter)\n",
    "    total_MSE = []\n",
    "    total_MAE = []\n",
    "    for i in range(iter):\n",
    "        index = test[i]\n",
    "\n",
    "        dense_tensor = data.get_historical_data(index, history_len)[0]\n",
    "        masks = data.get_historical_data(index, history_len)[1][:,index % data.num_masks,:,:]\n",
    "        sparse_tensor = dense_tensor * masks\n",
    "        \n",
    "        dense_tensor = dense_tensor.transpose(2,0,1)\n",
    "        sparse_tensor = sparse_tensor.transpose(2,0,1)\n",
    "        dim = dense_tensor.shape\n",
    "\n",
    "        start = time.time()\n",
    "        dim = np.array(sparse_tensor.shape)\n",
    "        rank = 20\n",
    "        vector = []\n",
    "        factor = []\n",
    "        for k in range(len(dim)):\n",
    "            vector.append(0.1 * np.random.randn(dim[k],))\n",
    "            factor.append(0.1 * np.random.randn(dim[k], rank))\n",
    "        burn_iter = 50\n",
    "        gibbs_iter = 50\n",
    "        _,_, total_MSE, total_MAPE=BGCP(dense_tensor, sparse_tensor,  factor, burn_iter, gibbs_iter)\n",
    "        end = time.time()\n",
    "        print('Running time: %d seconds'%(end - start))\n",
    "    return total_MSE,total_MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = r'D:\\WorkPath\\Models\\ImputeFormer'\n",
    "test_path = os.path.join(project_path , r'Data\\source_test_PEMS04') \n",
    "test_files = os.listdir(test_path)\n",
    "test_files = [os.path.join(test_path, file) for file in test_files]\n",
    "\n",
    "test_record = {'data_name':[],'MSE_test_loss':[] , 'MAPE_test_loss':[]}\n",
    "\n",
    "for file_path in test_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "\n",
    "    total_MSE,total_MAE=BGCP_T(test_data,5 , 3)\n",
    "    test_record['data_name'].append(file_path)\n",
    "    test_record['MSE_test_loss'].append(np.mean(total_MSE))\n",
    "    test_record['MAPE_test_loss'].append(np.mean(total_MAE))\n",
    "test_record = pd.DataFrame(test_record)\n",
    "test_record['route']=test_record['data_name'].apply(lambda x :x.split('_')[5])\n",
    "test_record['start']=test_record['data_name'].apply(lambda x :x.split('_')[-3])\n",
    "test_record['miss_rate']=test_record['data_name'].apply(lambda x :x.split('_')[-2])\n",
    "test_record['type']=test_record['data_name'].apply(lambda x :x.split('_')[-1][:-4])\n",
    "test_record=test_record[['route','start','miss_rate','type','MSE_test_loss','MAPE_test_loss']]\n",
    "test_record=test_record.sort_values(['route','type','start'])\n",
    "test_record"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepGAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
