{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset ,ConcatDataset\n",
    "from MyDataSet import MultiMaskTimeSeriesDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE_Mean(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,  dropout=0., act=F.relu):\n",
    "        '''\n",
    "        :param name: name of this layer\n",
    "        :param input_dim: input dimension\n",
    "        :param output_dim: output dimension\n",
    "        :param adj: adjacency matrix\n",
    "        :param dropout: dropout rate\n",
    "        \n",
    "        :param act: activation function \n",
    "        input: [batch_size, num_nodes, input_dim]   \n",
    "        output: [batch_size, num_nodes, output_dim]\n",
    "        \n",
    "        '''\n",
    "        super(GraphSAGE_Mean, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        \n",
    "        self.w1 = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        self.w2 = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        nn.init.xavier_uniform_(self.w1)\n",
    "        nn.init.xavier_uniform_(self.w2)\n",
    "    \n",
    "    def forward(self, inputs, adj):\n",
    "        inputs = F.dropout(inputs, p=self.dropout)\n",
    "        _self = torch.matmul(inputs, self.w1)\n",
    "        _neg = torch.matmul(torch.bmm(adj, inputs), self.w2)\n",
    "        \n",
    "        _concat = self.act(torch.cat([_self, _neg], dim=2))\n",
    "        return _concat\n",
    "    \n",
    "    \n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True, dropout=0., act=nn.ReLU()):\n",
    "        super(Linear, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.w = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        nn.init.xavier_uniform_(self.w)\n",
    "        self.use_bias = use_bias\n",
    "        if self.use_bias:\n",
    "            self.b = nn.Parameter(torch.Tensor(output_dim))\n",
    "            nn.init.zeros_(self.b)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.dropout(inputs, p=self.dropout, training=self.training)\n",
    "        x = torch.matmul(x, self.w)\n",
    "        if self.use_bias:\n",
    "            x = x + self.b\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, input_dim,out_dim,dropout=0.,act=nn.ReLU()):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layer1 = GraphSAGE_Mean(input_dim, out_dim, dropout=dropout,act=act)\n",
    "        self.layer2 = GraphSAGE_Mean(out_dim*2, out_dim, dropout=dropout,act=act)  \n",
    "    \n",
    "    def forward(self, inputs ,adj):\n",
    "        x = self.layer1(inputs,adj)\n",
    "        x = self.layer2(x,adj)\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, linear_hidden, dropout=0., act=nn.ReLU()):\n",
    "        super(Generator, self).__init__()\n",
    "        # 检测linear_hidden[0]是否为out_dim*4\n",
    "        assert linear_hidden[0] == output_dim*4, 'The first hidden layer should be out_dim*4'\n",
    "        \n",
    "        self.layer1 = GraphSAGE(input_dim, output_dim, dropout=dropout, act=act)\n",
    "        self.layer2 = GraphSAGE(input_dim, output_dim, dropout=dropout, act=act)\n",
    "        \n",
    "        self.h1 = Linear(linear_hidden[0],linear_hidden[1])\n",
    "        self.h2 = Linear(linear_hidden[1],linear_hidden[2])\n",
    "        self.h3 = Linear(linear_hidden[2],linear_hidden[-1],act=nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, inputs,adj,adj_ori):\n",
    "        concat_features_1 = self.layer1(inputs,adj)\n",
    "        concat_features_2 = self.layer2(inputs,adj_ori)\n",
    "        concat_features =  torch.cat([concat_features_1,concat_features_2], dim = -1)\n",
    "        x = self.h1(concat_features)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim,linear_hidden, dropout=0., act=nn.ReLU()):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.h1=Linear(input_dim,linear_hidden[0],dropout=dropout,act=act)\n",
    "        self.h2=Linear(linear_hidden[0],linear_hidden[1],dropout=dropout,act=act)\n",
    "        self.h3=Linear(linear_hidden[1],linear_hidden[2],dropout=dropout,act=lambda x:x)  \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.h1(inputs)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adj = torch.randn(3,5,5)\n",
    "adj_ori = torch.randn(3,5,5)\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "linear_hidden = [output_dim*4, 16, 1]\n",
    "dropout = 0.5\n",
    "act = nn.ReLU()\n",
    "gen = Generator(input_dim, output_dim, linear_hidden,  dropout, act)\n",
    "dis = Discriminator(output_dim*2, linear_hidden, dropout, act)\n",
    "print(gen(torch.randn(3,5,2), adj, adj_ori).shape)\n",
    "dis(torch.randn(3,5,4)).shape\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,Generator,  Discriminator,  dataloader , Gen_lr, Dis_lr, alpha ,p_hint,device):\n",
    "        '''\n",
    "        Generator: Generator model\n",
    "        Discriminator: Discriminator model\n",
    "        dataloader: DataLoader object\n",
    "        Gen_lr: Learning rate for the generator\n",
    "        Dis_lr: Learning rate for the discriminator\n",
    "        x_dim: Dimension of the input data\n",
    "        device: 'cuda' or 'cpu'\n",
    "        alpha: Hyperparameter for the generator loss\n",
    "        p_hint: Probability of hint vector\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.Generator = Generator\n",
    "        self.Discriminator = Discriminator\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.Gen_lr = Gen_lr\n",
    "        self.Dis_lr = Dis_lr\n",
    "        self.alpha = alpha\n",
    "        self.p_hint = p_hint\n",
    "        \n",
    "        self.optimizer_G = optim.RMSprop(Generator.parameters(), lr=Gen_lr)\n",
    "        self.optimizer_D = optim.RMSprop(Discriminator.parameters(), lr=Dis_lr)\n",
    "        \n",
    "        self.graph = {'iter':[],'G_loss': [], 'D_loss': [], 'MSE_loss': [], 'MSE_test_loss': []}\n",
    "        self.total_iter = 0\n",
    "        \n",
    "        # self.noise_z = self.sample_noise(x_dim)\n",
    "        \n",
    "    def sample_noise(self, dim):\n",
    "        return torch.rand(dim).to(self.device)\n",
    "    \n",
    "    # hint Vector Generation\n",
    "    def sample_hint(self, dim, p):\n",
    "        A=torch.rand(dim,dtype=torch.float16).to(self.device)\n",
    "        A[A>p]=1.0\n",
    "        A[A<=p]=0\n",
    "        return A\n",
    "    \n",
    "    def discriminator_loss(self,mask,x_mask ,x_raw ,adj ,adj_ori):\n",
    "        '''\n",
    "        mask: Masked data 1:raw, 0:gen\n",
    "        x_raw: Real data\n",
    "        Hint: Hint vector\n",
    "        '''\n",
    "        D_logit_real = self.Discriminator(x_raw)\n",
    "        G_samples = self.Generator(x_mask ,adj,adj_ori)\n",
    "        D_logit_fake = self.Discriminator(G_samples)\n",
    "        \n",
    "        d_loss_real = -torch.mean(D_logit_real)\n",
    "        d_loss_fake = torch.mean(D_logit_fake)\n",
    "        \n",
    "        mse_loss = torch.mean(( x_raw - G_samples)**2)\n",
    "        d_loss = d_loss_real + d_loss_fake \n",
    "        g_loss = -d_loss_fake + self.alpha * mse_loss\n",
    "        mse_test_loss = torch.mean(((1 - mask) * x_raw - (1 - mask) * G_samples)**2)/torch.mean(1 - mask)\n",
    "            \n",
    "        return d_loss, g_loss, mse_loss, mse_test_loss\n",
    "    \n",
    "\n",
    "    \n",
    "    def test(self , test_dataloader):\n",
    "        self.Generator.eval()\n",
    "        self.Discriminator.eval()\n",
    "        result = {'x_raw':[],'mask': [], 'G_loss': [], 'MSE_loss': [], 'MSE_test_loss': []}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_raw, mask,adj_ori,adj in test_dataloader:\n",
    "                x_raw = x_raw.float().to(self.device)\n",
    "                mask = mask.float().to(self.device)\n",
    "                adj_ori = adj_ori.float().to(self.device)\n",
    "                adj = adj.float().to(self.device)\n",
    "                x_raw = x_raw.permute(0,2,1)\n",
    "                mask = mask.permute(0,2,1)\n",
    "                \n",
    "                # print(x_raw.shape, mask.shape)\n",
    "                x_mask_noise = mask * x_raw \n",
    "                D_loss, G_loss, MSE_loss, MSE_test_loss = self.discriminator_loss(mask, x_mask_noise, x_raw, adj, adj_ori)\n",
    "                result['x_raw'].append(x_raw)\n",
    "                result['mask'].append(mask)\n",
    "                result['G_loss'].append(G_loss.item())\n",
    "                result['MSE_loss'].append(MSE_loss.item())\n",
    "                result['MSE_test_loss'].append(MSE_test_loss.item())\n",
    "\n",
    "\n",
    "                # print('Generator Loss: {:.4f}, MSE Train Loss: {:.4f}, MSE Test Loss: {:.4f}'.format(G_loss.item(), MSE_train_loss.item(), MSE_test_loss.item()))\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def train(self, epochs,save_path=None):\n",
    "        self.Generator.train()\n",
    "        self.Discriminator.train()\n",
    "        \n",
    "        print('Training...')\n",
    "        for it in tqdm(range(epochs)):\n",
    "            for x_raw, mask, adj_ori, adj in self.dataloader:\n",
    "                self.total_iter = self.total_iter + 1\n",
    "                x_raw = x_raw.float().to(self.device)\n",
    "                mask = mask.float().to(self.device)\n",
    "                adj_ori = adj_ori.float().to(self.device)\n",
    "                adj = adj.float().to(self.device)\n",
    "                \n",
    "                # 将x_raw和mask transform成【batch_size, num_nodes, time_dim】\n",
    "                x_raw = x_raw.permute(0,2,1)\n",
    "                mask = mask.permute(0,2,1)\n",
    "                \n",
    "                # print(x_raw.shape, mask.shape)\n",
    "                x_mask_noise = mask * x_raw \n",
    "                \n",
    "                # Train Discriminator\n",
    "                self.optimizer_D.zero_grad()\n",
    "                self.optimizer_G.zero_grad()\n",
    "                \n",
    "                D_loss, G_loss, MSE_loss,MSE_test_loss = self.discriminator_loss(mask, x_mask_noise, x_raw, adj, adj_ori)\n",
    "                D_loss.backward(retain_graph=True)\n",
    "                \n",
    "    \n",
    "                # Train Generator\n",
    "                G_loss.backward()\n",
    "                self.optimizer_D.step()\n",
    "                self.optimizer_G.step()\n",
    "                \n",
    "                if self.total_iter % 256 ==0:\n",
    "                    self.graph['iter'].append(self.total_iter)\n",
    "                    self.graph['G_loss'].append(G_loss.item())\n",
    "                    self.graph['D_loss'].append(D_loss.item())\n",
    "                    self.graph['MSE_loss'].append(MSE_loss.item())\n",
    "                    self.graph['MSE_test_loss'].append(MSE_test_loss.item())\n",
    "\n",
    "                \n",
    "                # print('MSE_train_loss:',MSE_train_loss.item())\n",
    "                \n",
    "                # break\n",
    "            \n",
    "            if it%2==0:\n",
    "                print('Epoch: {}, Generator Loss: {:.4f}, Discriminator Loss: {:.4f}, \\\n",
    "                      MSE Test Loss: {:.4f}'.format(it, G_loss.item(), \n",
    "                                                      D_loss.item(), MSE_test_loss.item()))\n",
    "                \n",
    "            if save_path is not None and (it+1)%20==0:\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                self.save_checkpoint(os.path.join(save_path,'checkpoint_{}.pth'.format(it)))\n",
    "        \n",
    "        if save_path is not None:\n",
    "            self.save_graph(os.path.join(save_path,'Train_record.pkl'))\n",
    "        \n",
    "        print('Training finished!')\n",
    "    \n",
    "    def save_checkpoint(self,path):\n",
    "        torch.save({'Generator_state_dict': self.Generator.state_dict(),\n",
    "                    'Discriminator_state_dict': self.Discriminator.state_dict(),\n",
    "                    'optimizer_G_state_dict': self.optimizer_G.state_dict(),\n",
    "                    'optimizer_D_state_dict': self.optimizer_D.state_dict()}, path)\n",
    "        \n",
    "    def save_graph(self,path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.graph, f)\n",
    "    \n",
    "    def load_checkpoint(self,path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.Generator.load_state_dict(checkpoint['Generator_state_dict'])\n",
    "        self.Discriminator.load_state_dict(checkpoint['Discriminator_state_dict'])\n",
    "        self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "        self.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "    \n",
    "    def plot_graph(self,graph):\n",
    "        # 分图绘制\n",
    "        plt.figure(figsize=(16, 9))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(graph['iter'], graph['G_loss'])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Generator Loss')\n",
    "        plt.title('Generator Loss')\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(graph['iter'], graph['D_loss'])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Discriminator Loss')\n",
    "        plt.title('Discriminator Loss')\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(graph['iter'], graph['MSE_loss'])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('MSE  Loss')\n",
    "        plt.title('MSE Loss')\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(graph['iter'], graph['MSE_test_loss'])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('MSE Test Loss')\n",
    "        plt.title('MSE Test Loss')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_path = r'D:\\heyulinWorkPath\\Models\\GA-GAN\\Data\\source_train'\n",
    "source_train_files = os.listdir(source_train_path)\n",
    "source_train_files = [os.path.join(source_train_path, file) for file in source_train_files]\n",
    "source_train_data = []\n",
    "\n",
    "for file_path in source_train_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    source_train_data.append(data)\n",
    "\n",
    "data = ConcatDataset(source_train_data)\n",
    "data_loader = DataLoader(data, batch_size=32, shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4*12 \n",
    "\n",
    "Discriminator = Discriminator(input_dim=input_dim,linear_hidden= [256, 64,1],\n",
    "                              dropout=0.0, act=nn.ReLU()).to(device)\n",
    "Generator = Generator(input_dim=input_dim, output_dim=64, \n",
    "                      linear_hidden=[64*4, 64*2, input_dim], dropout=0, act=nn.ReLU()).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(Generator, Discriminator, data_loader, Gen_lr=0.01, Dis_lr=0.01, alpha=100, p_hint=0.3, device=device)\n",
    "trainer.train(epochs=2, save_path=r'D:\\heyulinWorkPath\\Models\\GA-GAN\\Checkpoints')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepGAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
