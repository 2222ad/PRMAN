{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import math\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset ,ConcatDataset\n",
    "from Data.MyDataSet import MultiMaskTimeSeriesDataset\n",
    "sys.path.append(r'D:\\WorkPath\\Models\\ImputeFormer\\model')\n",
    "\n",
    "from ImputeFormer_main import ImputeFormer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedulerWithRestarts(LambdaLR):\n",
    "\n",
    "    def __init__(self, optimizer: Optimizer,\n",
    "                 num_warmup_steps: int,\n",
    "                 num_training_steps: int,\n",
    "                 min_factor: float = 0.1,\n",
    "                 linear_decay: float = 0.67,\n",
    "                 num_cycles: int = 1,\n",
    "                 last_epoch: int = -1):\n",
    "        \"\"\"From https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/optimization.py#L138\n",
    "\n",
    "        Create a schedule with a learning rate that decreases following the values\n",
    "        of the cosine function between the initial lr set in the optimizer to 0,\n",
    "        with several hard restarts, after a warmup period during which it increases\n",
    "        linearly between 0 and the initial lr set in the optimizer.\n",
    "\n",
    "        Args:\n",
    "            optimizer ([`~torch.optim.Optimizer`]):\n",
    "                The optimizer for which to schedule the learning rate.\n",
    "            num_warmup_steps (`int`):\n",
    "                The number of steps for the warmup phase.\n",
    "            num_training_steps (`int`):\n",
    "                The total number of training steps.\n",
    "            num_cycles (`int`, *optional*, defaults to 1):\n",
    "                The number of hard restarts to use.\n",
    "            last_epoch (`int`, *optional*, defaults to -1):\n",
    "                The index of the last epoch when resuming training.\n",
    "        Return:\n",
    "            `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "        \"\"\"\n",
    "\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < num_warmup_steps:\n",
    "                factor = float(current_step) / float(max(1, num_warmup_steps))\n",
    "                return max(min_factor, factor)\n",
    "            progress = float(current_step - num_warmup_steps)\n",
    "            progress /= float(max(1, num_training_steps - num_warmup_steps))\n",
    "            if progress >= 1.0:\n",
    "                return 0.0\n",
    "            factor = (float(num_cycles) * progress) % 1.0\n",
    "            cos = 0.5 * (1.0 + math.cos(math.pi * factor))\n",
    "            lin = 1.0 - (progress * linear_decay)\n",
    "            return max(min_factor, cos * lin)\n",
    "\n",
    "        super(CosineSchedulerWithRestarts, self).__init__(optimizer, lr_lambda,\n",
    "                                                          last_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer_Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,Generator,  Discriminator,  dataloader , Gen_lr, Dis_lr, alpha ,p_hint ,device , lr_scheduler=None , gamma = 0.2):\n",
    "        '''\n",
    "        Generator: Generator model\n",
    "        Discriminator: Discriminator model\n",
    "        dataloader: DataLoader object\n",
    "        Gen_lr: Learning rate for the generator\n",
    "        Dis_lr: Learning rate for the discriminator\n",
    "        x_dim: Dimension of the input data\n",
    "        device: 'cuda' or 'cpu'\n",
    "        alpha: Hyperparameter for the generator loss\n",
    "        p_hint: Probability of hint vector\n",
    "        lr_scheduler: Learning rate scheduler\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.Generator = Generator\n",
    "        self.Discriminator = Discriminator\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.Gen_lr = Gen_lr\n",
    "        self.Dis_lr = Dis_lr\n",
    "        self.alpha = alpha\n",
    "        self.p_hint = p_hint\n",
    "        \n",
    "        self.optimizer_G = optim.Adam(Generator.parameters(), lr=Gen_lr, betas=(0, 0.9))\n",
    "        self.optimizer_D = optim.Adam(Discriminator.parameters(), lr=Dis_lr, betas=(0, 0.9))\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            self.lr_scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer_G, mode='min',threshold=0.00005, factor=gamma, patience=10, verbose=True)\n",
    "            self.lr_scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer_D, mode='min',threshold=0.00005, factor=gamma, patience=10, verbose=True)\n",
    "        \n",
    "        \n",
    "        self.graph = {'iter':[],'G_loss': [], 'D_loss': [], 'MSE_train_loss': [], 'MSE_test_loss': []}\n",
    "        self.total_iter = 0\n",
    "        \n",
    "        # self.noise_z = self.sample_noise(x_dim)\n",
    "        \n",
    "    def sample_noise(self, dim):\n",
    "        return torch.rand(dim).to(self.device)\n",
    "    \n",
    "    # hint Vector Generation\n",
    "    def sample_hint(self, dim, p):\n",
    "        A=torch.rand(dim,dtype=torch.float16).to(self.device)\n",
    "        A[A>p]=1.0\n",
    "        A[A<=p]=0\n",
    "        return A\n",
    "    \n",
    "    def discriminator_loss(self,mask,x_mask_noise,Hint):\n",
    "        '''\n",
    "        mask: Masked data 1:raw, 0:gen\n",
    "        x_raw: Real data\n",
    "        Hint: Hint vector\n",
    "        '''\n",
    "        x_input = torch.cat([x_mask_noise,mask], dim=1).float()\n",
    "        x_gen = self.Generator(x_input)\n",
    "        \n",
    "        x_Hat = mask * x_mask_noise + (1 - mask) * x_gen\n",
    "        \n",
    "        D_prob = self.Discriminator(torch.cat([x_Hat,Hint], dim=1))\n",
    "        \n",
    "        return -torch.mean(mask * torch.log(D_prob + 1e-8) + (1 - mask) * torch.log(1.0 - D_prob + 1e-8))\n",
    "    \n",
    "    def generator_loss(self,x_raw,mask,x_mask_noise,Hint):\n",
    "        '''\n",
    "        x_raw: Real data\n",
    "        x_gen: Generated data\n",
    "        mask: Masked data 1:raw, 0:gen\n",
    "        x_Hat: Combined data : mask * x_raw + (1 - mask) * x_gen\n",
    "        D_prob: Discriminator output\n",
    "        '''\n",
    "        x_input = torch.cat([x_mask_noise,mask], dim=1)\n",
    "        x_gen = self.Generator(x_input)\n",
    "        x_Hat = mask * x_mask_noise + (1 - mask) * x_gen\n",
    "        D_prob = self.Discriminator(torch.cat([x_Hat,Hint], dim=1))\n",
    "        \n",
    "        \n",
    "        G_loss1 = -torch.mean((1 - mask) * torch.log(D_prob + 1e-8))\n",
    "        MSE_train_loss = torch.mean((mask * x_mask_noise - mask * x_gen)**2)/torch.mean(mask)\n",
    "        G_loss = G_loss1 + self.alpha * MSE_train_loss\n",
    "        MSE_test_loss = torch.mean(((1 - mask) * x_raw - (1 - mask) * x_gen)**2)/torch.mean(1 - mask)\n",
    "        \n",
    "        return G_loss, MSE_train_loss, MSE_test_loss\n",
    "    \n",
    "    def test(self , test_dataloader ):\n",
    "        self.Generator.eval()\n",
    "        self.Discriminator.eval()\n",
    "        result = {'x_raw':[],'mask': [], 'G_loss': [], 'MSE_train_loss': [], 'MSE_test_loss': []}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_raw, mask in test_dataloader:\n",
    "                x_raw = x_raw.float().to(self.device).unsqueeze(-1)\n",
    "                mask = mask.float().to(self.device).unsqueeze(-1) \n",
    "                x_mask_noise = mask * x_raw + (1 - mask) * self.sample_noise(x_raw.shape).to(self.device)\n",
    "\n",
    "                G_loss, MSE_train_loss, MSE_test_loss = self.generator_loss(x_raw, mask, x_mask_noise, mask)\n",
    "                result['x_raw'].append(x_raw)\n",
    "                result['mask'].append(mask)\n",
    "                result['G_loss'].append(G_loss.item())\n",
    "                result['MSE_train_loss'].append(MSE_train_loss.item())\n",
    "                result['MSE_test_loss'].append(MSE_test_loss.item())\n",
    "\n",
    "                # print('Generator Loss: {:.4f}, MSE Train Loss: {:.4f}, MSE Test Loss: {:.4f}'.format(G_loss.item(), MSE_train_loss.item(), MSE_test_loss.item()))\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def train(self, epochs,save_path=None , test_dataloader=None):\n",
    "        self.Generator.train()\n",
    "        self.Discriminator.train()\n",
    "        \n",
    "        # 如果存在test_dataloader,则保存测试结果\n",
    "        if test_dataloader is not None:\n",
    "            test_graph = {'test_epochs':[],'test_G_loss': [], 'test_MSE_train_loss': [], 'test_MSE_test_loss': [] ,'test_lr':[]}\n",
    "\n",
    "        print('Training...')\n",
    "        for it in tqdm(range(epochs)):\n",
    "            for x_raw, mask in self.dataloader:\n",
    "                self.total_iter = self.total_iter + 1\n",
    "                x_raw = x_raw.float().to(self.device).unsqueeze(1)\n",
    "                mask = mask.float().to(self.device).unsqueeze(1)\n",
    "                \n",
    "                # print(x_raw.shape, mask.shape)\n",
    "                x_mask_noise = mask * x_raw + (1 - mask) * self.sample_noise(x_raw.shape).to(self.device)\n",
    "                hint = self.sample_hint(mask.shape, 1.0-self.p_hint)\n",
    "                mask_hint = mask * hint\n",
    "                \n",
    "                # Train Discriminator\n",
    "                self.optimizer_D.zero_grad()\n",
    "                D_loss = self.discriminator_loss(mask,x_mask_noise,mask_hint)\n",
    "                D_loss.backward()\n",
    "                self.optimizer_D.step()\n",
    "                \n",
    "                # Train Generator\n",
    "                self.optimizer_G.zero_grad()\n",
    "                G_loss, MSE_train_loss, MSE_test_loss = self.generator_loss(x_raw, mask, x_mask_noise, mask_hint)\n",
    "                G_loss.backward()\n",
    "                self.optimizer_G.step()\n",
    "                \n",
    "                if self.total_iter % 256 ==0:\n",
    "                    self.graph['iter'].append(self.total_iter)\n",
    "                    self.graph['G_loss'].append(G_loss.item())\n",
    "                    self.graph['D_loss'].append(D_loss.item())\n",
    "                    self.graph['MSE_train_loss'].append(MSE_train_loss.item())\n",
    "                    self.graph['MSE_test_loss'].append(MSE_test_loss.item())\n",
    "                \n",
    "                # print('MSE_train_loss:',MSE_train_loss.item())\n",
    "                \n",
    "                # break\n",
    "            if test_dataloader is not None:\n",
    "                result = self.test(test_dataloader)\n",
    "                self.lr_scheduler_D.step(np.mean(result['MSE_test_loss']))\n",
    "                self.lr_scheduler_G.step(np.mean(result['MSE_test_loss']))\n",
    "                \n",
    "                test_graph['test_epochs'].append(epochs)\n",
    "                test_graph['test_G_loss'].append(np.mean(result['G_loss']))\n",
    "                test_graph['test_MSE_train_loss'].append(np.mean(result['MSE_train_loss']))\n",
    "                test_graph['test_MSE_test_loss'].append(np.mean(result['MSE_test_loss']))\n",
    "                test_graph['test_lr'].append(self.optimizer_G.param_groups[0]['lr'])\n",
    "                \n",
    "            \n",
    "            if it%2==0:\n",
    "                print('Epoch: {}, Generator Loss: {:.4f}, Discriminator Loss: {:.4f}, MSE Train Loss: {:.4f}, \\\n",
    "                        MSE Test Loss: {:.4f}, lr :{}'.format(it, G_loss.item(), D_loss.item(), MSE_train_loss.item(), MSE_test_loss.item(), self.optimizer_G.param_groups[0]['lr']) )\n",
    "                \n",
    "            if save_path is not None and (it+1)%20==0:\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                self.save_checkpoint(os.path.join(save_path,'checkpoint_{}.pth'.format(it)))\n",
    "        \n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "            self.save_graph(os.path.join(save_path,'Train_record.pkl'))\n",
    "        \n",
    "        print('Training finished!')\n",
    "        \n",
    "        if test_dataloader is not None:\n",
    "            return test_graph\n",
    "    \n",
    "    def save_checkpoint(self,path):\n",
    "        torch.save({'Generator_state_dict': self.Generator.state_dict(),\n",
    "                    'Discriminator_state_dict': self.Discriminator.state_dict(),\n",
    "                    'optimizer_G_state_dict': self.optimizer_G.state_dict(),\n",
    "                    'optimizer_D_state_dict': self.optimizer_D.state_dict()}, path)\n",
    "        \n",
    "    def save_graph(self,path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.graph, f)\n",
    "    \n",
    "    def load_checkpoint(self,path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.Generator.load_state_dict(checkpoint['Generator_state_dict'])\n",
    "        self.Discriminator.load_state_dict(checkpoint['Discriminator_state_dict'])\n",
    "        self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "        self.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "    \n",
    "    def plot_graph(self,graph):\n",
    "        \n",
    "        plt.figure(figsize=(16, 9))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(graph['iter'], graph['G_loss'])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Generator Loss')\n",
    "        plt.title('Generator Loss')\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(graph['iter'], graph['D_loss'])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Discriminator Loss')\n",
    "        plt.title('Discriminator Loss')\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(graph['iter'], graph['MSE_train_loss'])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('MSE Train Loss')\n",
    "        plt.title('MSE Train Loss')\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(graph['iter'], graph['MSE_test_loss'])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('MSE Test Loss')\n",
    "        plt.title('MSE Test Loss')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_test_graph(self,graph):\n",
    "        plt.figure(figsize=(16, 9))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(graph['test_epochs'], graph['test_G_loss'])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('TestSet Generator Loss')\n",
    "        plt.title('TestSet Generator Loss')\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(graph['test_epochs'], graph['test_MSE_train_loss'])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('TestSet MSE Train Loss')\n",
    "        plt.title('TestSet MSE Train Loss')\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(graph['test_epochs'], graph['test_MSE_test_loss'])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('TestSet MSE Test Loss')\n",
    "        plt.title('TestSet MSE Test Loss')\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(graph['test_epochs'], graph['test_lr'])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('TestSet Learning Rate')\n",
    "        plt.title('TestSet Learning Rate')\n",
    "        \n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer_noGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trainer_noGAN\n",
    "\n",
    "class Trainer_NOGAN():\n",
    "    def __init__(self,Generator,    dataloader , Gen_lr ,p_hint ,device , lr_scheduler=None , gamma = 0.2 ,No_impute_wight=1.0):\n",
    "        '''\n",
    "        Generator: Generator model\n",
    "        Discriminator: Discriminator model\n",
    "        dataloader: DataLoader object\n",
    "        Gen_lr: Learning rate for the generator\n",
    "        Dis_lr: Learning rate for the discriminator\n",
    "        x_dim: Dimension of the input data\n",
    "        device: 'cuda' or 'cpu'\n",
    "        alpha: Hyperparameter for the generator loss\n",
    "        p_hint: Probability of hint vector\n",
    "        lr_scheduler: Learning rate scheduler\n",
    "        '''\n",
    "        self.Generator = Generator\n",
    "\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.Gen_lr = Gen_lr\n",
    "        self.p_hint = p_hint\n",
    "        self.No_impute_wight=No_impute_wight\n",
    "        \n",
    "        self.optimizer_G = optim.Adam(Generator.parameters(), lr=Gen_lr, betas=(0, 0.9))\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            self.lr_scheduler_G = CosineSchedulerWithRestarts(self.optimizer_G, num_cycles=300//100,num_training_steps=300,\n",
    "                                                              num_warmup_steps=12, linear_decay=0.67,min_factor=0.1)\n",
    "        else :\n",
    "            self.lr_scheduler_G = None\n",
    "        \n",
    "        self.graph = {'iter':[],'G_loss': [], 'D_loss': [], 'MSE_train_loss': [], 'MSE_test_loss': []}\n",
    "        self.total_iter = 0\n",
    "        \n",
    "        # self.noise_z = self.sample_noise(x_dim)\n",
    "        \n",
    "    def sample_noise(self, dim):\n",
    "        return torch.rand(dim).to(self.device)*0.1\n",
    "    \n",
    "    # hint Vector Generation\n",
    "    def sample_hint(self, dim, p):\n",
    "        A=torch.rand(dim,dtype=torch.float16).to(self.device)\n",
    "        A[A>p]=1.0\n",
    "        A[A<=p]=0\n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def generator_loss(self,x_raw,mask,x_mask_noise,u,Hint):\n",
    "        '''\n",
    "        x_raw: Real data\n",
    "        x_gen: Generated data\n",
    "        mask: Masked data 1:raw, 0:gen\n",
    "        x_Hat: Combined data : mask * x_raw + (1 - mask) * x_gen\n",
    "        '''\n",
    "        # x_input = torch.cat([x_mask_noise,mask], dim=1).float()\n",
    "        x_gen = self.Generator(x_raw,u,mask)\n",
    "        MSE_train_loss = torch.mean((mask * x_mask_noise - mask * x_gen)**2)/torch.mean(mask)\n",
    "        MSE_test_loss = torch.mean(((1 - mask) * x_raw - (1 - mask) * x_gen)**2)/torch.mean(1 - mask)\n",
    "        if Hint is not None:\n",
    "            temp= (1-mask).bool()\n",
    "            MAPE_test_loss = torch.mean(torch.abs(( x_raw[temp] -  x_gen[temp])/ (x_raw[temp]+1e-2) ))\n",
    "        else :\n",
    "            MAPE_test_loss = None\n",
    "        return  MSE_train_loss, MSE_test_loss ,MAPE_test_loss\n",
    "    \n",
    "    def test(self , test_dataloader ):\n",
    "        self.Generator.eval()\n",
    "        result = {'x_raw':[],'mask': [], 'G_loss': [], 'MSE_train_loss': [], 'MSE_test_loss': [], 'MAPE_test_loss': []}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_raw,u, mask in test_dataloader:\n",
    "                x_raw = x_raw.float().to(self.device).unsqueeze(-1)\n",
    "                u = u.float().to(self.device)\n",
    "                mask = mask.float().to(self.device).unsqueeze(-1) \n",
    "                x_mask_noise = mask * x_raw + (1 - mask) * self.sample_noise(x_raw.shape).to(self.device)\n",
    "\n",
    "                MSE_train_loss, MSE_test_loss, MAPE_test_loss = self.generator_loss(x_raw, mask, x_mask_noise,u, mask)\n",
    "                result['x_raw'].append(x_raw)\n",
    "                result['mask'].append(mask)\n",
    "                result['MSE_train_loss'].append(MSE_train_loss.item())\n",
    "                result['MSE_test_loss'].append(MSE_test_loss.item())\n",
    "                result['MAPE_test_loss'].append(MAPE_test_loss .item())\n",
    "                # print('Generator Loss: {:.4f}, MSE Train Loss: {:.4f}, MSE Test Loss: {:.4f}'.format(G_loss.item(), MSE_train_loss.item(), MSE_test_loss.item()))\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def train(self, epochs,save_path=None , test_dataloader=None):\n",
    "        self.Generator.train()\n",
    "        \n",
    "        if test_dataloader is not None:\n",
    "            test_graph = {'test_epochs':[], 'test_MSE_train_loss': [], 'test_MSE_test_loss': [] ,'test_lr':[]}\n",
    "\n",
    "        print('Training...')\n",
    "        for it in tqdm(range(epochs)):\n",
    "            for x_raw,u, mask in self.dataloader:\n",
    "                self.total_iter = self.total_iter + 1\n",
    "                x_raw = x_raw.float().to(self.device).unsqueeze(-1)\n",
    "                u = u.float().to(self.device)\n",
    "                mask = mask.float().to(self.device).unsqueeze(-1)\n",
    "                \n",
    "                # print(x_raw.shape, mask.shape)\n",
    "                x_mask_noise = mask * x_raw + (1 - mask) * self.sample_noise(x_raw.shape).to(self.device)\n",
    "                # hint = self.sample_hint(mask.shape, 1.0-self.p_hint)\n",
    "                # mask_hint = mask * hint\n",
    "                \n",
    "                \n",
    "                # Train Generator\n",
    "                self.optimizer_G.zero_grad()\n",
    "                MSE_train_loss, MSE_test_loss,_ = self.generator_loss(x_raw, mask, x_mask_noise, u, None)\n",
    "                total_loss = MSE_train_loss + self.No_impute_wight*MSE_test_loss\n",
    "                total_loss.backward()\n",
    "                self.optimizer_G.step()\n",
    "                \n",
    "                if self.lr_scheduler_G is not None:\n",
    "                    self.lr_scheduler_G.step()\n",
    "                \n",
    "                if self.total_iter % 256 ==0:\n",
    "                    self.graph['iter'].append(self.total_iter)\n",
    "                    self.graph['MSE_train_loss'].append(MSE_train_loss.item())\n",
    "                    self.graph['MSE_test_loss'].append(MSE_test_loss.item())\n",
    "                \n",
    "                # print('MSE_train_loss:',MSE_train_loss.item())\n",
    "                \n",
    "                # break\n",
    "            if test_dataloader is not None:\n",
    "                result = self.test(test_dataloader)\n",
    "                self.lr_scheduler_G.step(np.mean(result['MSE_test_loss']))\n",
    "                \n",
    "                test_graph['test_epochs'].append(epochs)\n",
    "                test_graph['test_MSE_train_loss'].append(np.mean(result['MSE_train_loss']))\n",
    "                test_graph['test_MSE_test_loss'].append(np.mean(result['MSE_test_loss']))\n",
    "                test_graph['test_lr'].append(self.optimizer_G.param_groups[0]['lr'])\n",
    "                \n",
    "            \n",
    "            if it%2==0:\n",
    "                print('Epoch: {}, Generator Loss: {:.4f}, MSE Train Loss: {:.4f},  MSE Test Loss: {:.4f}, lr :{}' \\\n",
    "                      .format(it, total_loss.item(), MSE_train_loss.item(), MSE_test_loss.item(), self.optimizer_G.param_groups[0]['lr']) )\n",
    "                \n",
    "            if save_path is not None and (it+1)%20==0:\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                self.save_checkpoint(os.path.join(save_path,'checkpoint_{}.pth'.format(it)))\n",
    "        \n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "            self.save_graph(os.path.join(save_path,'Train_record.pkl'))\n",
    "        \n",
    "        print('Training finished!')\n",
    "        \n",
    "        if test_dataloader is not None:\n",
    "            return test_graph\n",
    "    \n",
    "    def save_checkpoint(self,path):\n",
    "        torch.save({'Generator_state_dict': self.Generator.state_dict(),\n",
    "                    'optimizer_G_state_dict': self.optimizer_G.state_dict()}, path)\n",
    "        \n",
    "    def save_graph(self,path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.graph, f)\n",
    "    \n",
    "    def load_checkpoint(self,path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.Generator.load_state_dict(checkpoint['Generator_state_dict'])\n",
    "        self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "\n",
    "    def plot_graph(self,graph):\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(16, 9))\n",
    "        # plt.subplot(2, 2, 1)\n",
    "        # plt.plot(graph['iter'], graph['G_loss'])\n",
    "        # plt.xlabel('Iteration')\n",
    "        # plt.ylabel('Generator Loss')\n",
    "        # plt.title('Generator Loss')\n",
    "        \n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(graph['iter'], graph['MSE_train_loss'])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('MSE Train Loss')\n",
    "        plt.title('MSE Train Loss')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(graph['iter'], graph['MSE_test_loss'])\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('MSE Test Loss')\n",
    "        plt.title('MSE Test Loss')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_test_graph(self,graph):\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(16, 9))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(graph['test_epochs'], graph['test_MSE_train_loss'])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('TestSet MSE Train Loss')\n",
    "        plt.title('TestSet MSE Train Loss')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(graph['test_epochs'], graph['test_MSE_test_loss'])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('TestSet MSE Test Loss')\n",
    "        plt.title('TestSet MSE Test Loss')\n",
    "        \n",
    "        # plt.subplot(2, 2, 4)\n",
    "        # plt.plot(graph['test_epochs'], graph['test_lr'])\n",
    "        # plt.xlabel('Epochs')\n",
    "        # plt.ylabel('TestSet Learning Rate')\n",
    "        # plt.title('TestSet Learning Rate')\n",
    "        \n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_path = r'D:\\WorkPath\\Models\\ImputeFormer\\Data\\source_train_SanDiego'\n",
    "source_train_files = os.listdir(source_train_path)\n",
    "source_train_files = [os.path.join(source_train_path, file) for file in source_train_files]\n",
    "source_train_data = []\n",
    "\n",
    "for file_path in source_train_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    source_train_data.append(data)\n",
    "\n",
    "data = ConcatDataset(source_train_data)\n",
    "data_loader = DataLoader(data, batch_size=32, shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(len(data) * 0.8)\n",
    "# test_size = len(data) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "# data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# print(len(train_dataset))\n",
    "# print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training_NoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = ImputeFormer(num_nodes=32,input_dim=3,output_dim=1,input_embedding_dim=32,\n",
    "                     learnable_embedding_dim=96, feed_forward_dim=256,num_temporal_heads=4,\n",
    "                     num_layers= 3,dim_proj= 8,windows=48).to(device)\n",
    "\n",
    "lr=0.0001\n",
    "trainer = Trainer_NOGAN(model,data_loader,Gen_lr=lr,p_hint=0,device=device,lr_scheduler=None,No_impute_wight=1)\n",
    "# trainer.load_checkpoint(r'D:\\WorkPath\\Models\\ImputeFormer\\Checkpoints\\checkpoint_119.pth')\n",
    "# trainer.train(epochs=40, save_path=r'D:\\WorkPath\\Models\\ImputeFormer\\Checkpoints') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = r'D:\\WorkPath\\Models\\ImputeFormer'\n",
    "test_path = os.path.join(project_path , r'Data\\source_test_PEMS04') \n",
    "test_files = os.listdir(test_path)\n",
    "test_files = [os.path.join(test_path, file) for file in test_files]\n",
    "test_record = {'data_name':[],'MSE_train_loss':[] , 'MSE_test_loss':[], 'MAPE_test_loss':[]}\n",
    "\n",
    "trainer.load_checkpoint(r'D:\\WorkPath\\Models\\ImputeFormer\\SAVE\\checkpoint_20240918.pth')\n",
    "\n",
    "for file_path in test_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "    result=trainer.test(test_data_loader)\n",
    "    test_record['data_name'].append(file_path)\n",
    "    test_record['MSE_train_loss'].append(np.mean(result['MSE_train_loss'][1:]))\n",
    "    test_record['MSE_test_loss'].append(np.mean(result['MSE_test_loss'][1:]))\n",
    "    test_record['MAPE_test_loss'].append(np.mean(result['MAPE_test_loss'][1:]))\n",
    "test_record = pd.DataFrame(test_record)\n",
    "test_record['route']=test_record['data_name'].apply(lambda x :x.split('_')[5])\n",
    "test_record['start']=test_record['data_name'].apply(lambda x :x.split('_')[-3])\n",
    "test_record['miss_rate']=test_record['data_name'].apply(lambda x :x.split('_')[-2])\n",
    "test_record['type']=test_record['data_name'].apply(lambda x :x.split('_')[-1][:-4])\n",
    "test_record=test_record[['route','start','miss_rate','type','MSE_train_loss','MSE_test_loss','MAPE_test_loss']]\n",
    "test_record=test_record.sort_values(['route','type','start'])\n",
    "test_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(r'D:\\WorkPath\\Models\\ImputeFormer\\SAVE\\checkpoint_20241007_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_record.groupby('type')[['MSE_test_loss','MAPE_test_loss']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = ImputeFormer(num_nodes=32,input_dim=3,output_dim=1,input_embedding_dim=32,\n",
    "                     learnable_embedding_dim=96, feed_forward_dim=256,num_temporal_heads=4,\n",
    "                     num_layers= 3,dim_proj= 8,windows=48).to(device)\n",
    "\n",
    "lr=0.0001\n",
    "trainer = Trainer_NOGAN(model,data_loader,Gen_lr=lr,p_hint=0,device=device,lr_scheduler=None,No_impute_wight=1)\n",
    "\n",
    "project_path = r'D:\\WorkPath\\Models\\ImputeFormer'\n",
    "test_path = os.path.join(project_path , r'Data\\target_test') \n",
    "test_files = os.listdir(test_path)\n",
    "test_files = [os.path.join(test_path, file) for file in test_files]\n",
    "test_record = {'data_name':[],'MSE_train_loss':[] , 'MSE_test_loss':[], 'MAPE_test_loss':[]}\n",
    "\n",
    "trainer.load_checkpoint(r'D:\\WorkPath\\Models\\ImputeFormer\\Checkpoints\\checkpoint_199.pth')\n",
    "\n",
    "for file_path in test_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "    result=trainer.test(test_data_loader)\n",
    "    test_record['data_name'].append(file_path)\n",
    "    test_record['MSE_train_loss'].append(np.mean(result['MSE_train_loss'][1:]))\n",
    "    test_record['MSE_test_loss'].append(np.mean(result['MSE_test_loss'][1:]))\n",
    "    test_record['MAPE_test_loss'].append(np.mean(result['MAPE_test_loss'][1:]))\n",
    "test_record = pd.DataFrame(test_record)\n",
    "test_record['route']=test_record['data_name'].apply(lambda x :x.split('_')[4])\n",
    "test_record['start']=test_record['data_name'].apply(lambda x :x.split('_')[-3])\n",
    "test_record['miss_rate']=test_record['data_name'].apply(lambda x :x.split('_')[-2])\n",
    "test_record['type']=test_record['data_name'].apply(lambda x :x.split('_')[-1][:-4])\n",
    "test_record=test_record[['route','start','miss_rate','type','MSE_train_loss','MSE_test_loss','MAPE_test_loss']]\n",
    "test_record=test_record.sort_values(['route','type','start'])\n",
    "test_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_record['MSE_test_loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_record.groupby('type')[['MSE_test_loss','MAPE_test_loss']].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepGAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
